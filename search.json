[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Using open source data for web maps\n\n\n\n\n\n\nopenstreetmap\n\n\nmapping\n\n\n\nIf one day you want a map of all the tennis courts in an area\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex from scratch\n\n\n\n\n\n\nrstats\n\n\nindexes\n\n\n\nGetting started with indexes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular expressions\n\n\n\n\n\n\nrstats\n\n\nregex\n\n\ndata_engineering\n\n\n\nExtracting patterns of characters from noisy inputs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nError handling in R\n\n\n\n\n\n\nrstats\n\n\nerrors\n\n\nexceptions\n\n\nfunctions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb scrapping in R\n\n\n\n\n\n\nscrapping\n\n\nrstats\n\n\nmapping\n\n\n\nWhen information is hard to get a hand on, consider scrapping it\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFast Distance rewriting in R with C++\n\n\n\n\n\n\ncpp\n\n\nrstats\n\n\ngis\n\n\n\nMaking great circle distance computations fast.\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fast_distance/fast_distance.html",
    "href": "posts/fast_distance/fast_distance.html",
    "title": "Fast Distance rewriting in R with C++",
    "section": "",
    "text": "Everyone in the R community is probably familiar with the great sf package. It allows you to do pretty much any operations with geometries, and works very well with the tidyverse packages and data.table."
  },
  {
    "objectID": "posts/fast_distance/fast_distance.html#rewriting-numerically-in-c",
    "href": "posts/fast_distance/fast_distance.html#rewriting-numerically-in-c",
    "title": "Fast Distance rewriting in R with C++",
    "section": "Rewriting numerically in C++",
    "text": "Rewriting numerically in C++\nThe following code is vectorised so the functions can be written to compute the distance between two sets of points, provided as matrices with the x coordinate in the first column and y in the second. Notice that the angle values are converted to radians using the following relation \\(\\theta_{rad} = \\frac{\\theta_{deg}*\\pi}{180}\\)\n\n#include &lt;RcppArmadillo.h&gt;\n#include &lt;cstdlib&gt;\n\n// [[Rcpp::depends(RcppArmadillo)]]\n// [[Rcpp::plugins(\"cpp11\")]]\n\nconst double radius = 6371009;\nconst double pi = 3.141593;\n\n// [[Rcpp::export]]\narma::vec gc_distance_pair_cpp(const arma::mat& coord1,const arma::mat& coord2) {\n\n  arma::uword n = coord2.n_rows;\n\n  arma::vec x1(n);\n  arma::vec y1(n);\n\n  arma::vec x2(n);\n  arma::vec y2(n);\n\n  x2 = coord2.col(0)*pi/180;\n  y2 = coord2.col(1)*pi/180;\n\n  x1 = coord1.col(0)*pi/180;\n  y1 = coord1.col(1)*pi/180;\n\n  arma::vec num = arma::sqrt(arma::square(arma::cos(y2)%arma::sin(arma::abs(x1-x2))) + arma::square(arma::cos(y1)%sin(y2) - arma::sin(y1)%arma::cos(y2)%arma::cos(arma::abs(x1-x2))));\n  arma::vec den = arma::sin(y1)%arma::sin(y2) + arma::cos(y1)%arma::cos(y2)%arma::cos(arma::abs(x1-x2));\n\n  return radius*arma::atan(num/den);\n}"
  },
  {
    "objectID": "posts/error_handling/error_handling_r.html",
    "href": "posts/error_handling/error_handling_r.html",
    "title": "Error handling in R",
    "section": "",
    "text": "This blog post will cover exception/error handling in R. The main purpose of it is to ultimately build robust, fault proof scripts, that can be part of any workflow and be easily shared with others, via packages or version control and perform as expected in any situation or raise informative messages to the user as to the possible problems.\nWhether you are a researcher, a student, a software engineer, a data scientist, you will always encounter numerous bugs, errors and other types of unexpected behavior from your machine when coding. And while this is pretty much inevitable, there are good habits and tricks to learn in order to reduce the time spent figuring out, solving problems and anticipating troubles. Ultimately, the goal is to build efficient code, that is well commented, fault proof, easily reusable. All these aspects are important when developing software for personal use and be shared and used by others.\nSo this post will focus on common mistakes that are encountered, the main things to keep in mind, how to build functions that do specifically what you ask them to every time, or notify you of what is wrong.\nThis tutorial will provide examples in R, but the ideas covered apply more generally to problem solving and software development in many languages."
  },
  {
    "objectID": "posts/error_handling/error_handling_r.html#helper-functions",
    "href": "posts/error_handling/error_handling_r.html#helper-functions",
    "title": "Error handling in R",
    "section": "Helper functions",
    "text": "Helper functions\nIn R, a mechanism is implemented to give the programmer access to the error logs from the low level code. Errors are returned as objects of class try-error and can be handled through the tryCatch function. It will attempt to run an expression provided as argument, and will catch the error objects in case of failures and provide access to the internals of the error. Let’s look at it on the example of the previously defined function\n\ntryCatch(\n  expr = square('3')\n  ,error = function(e) str(e)\n  ,warning = function(w) w\n)\n\nList of 2\n $ message: chr \"could not find function \\\"square\\\"\"\n $ call   : language square(\"3\")\n - attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\n\n\nThis function is the backbone of error handling in R and it also has a few wrapper functions that help depending on the context of usage as we will see shortly. Now in order to make our original function more fault proof and usable, let’s try to resolve potential issues, such as for example passing a string containing a number:\n\nsquare_better &lt;- function(x){\n  if(inherits(x,'character')) {\n    warning('attempting to convert input to numeric')\n    x &lt;- as.numeric(x)\n  }\n  return(x**2)\n}\n\ntryCatch(\n  expr = square_better('3')\n  ,error = function(e) str(e)\n  ,warning = function(w) str(w)\n)\n\nList of 2\n $ message: chr \"attempting to convert input to numeric\"\n $ call   : language square_better(\"3\")\n - attr(*, \"class\")= chr [1:3] \"simpleWarning\" \"warning\" \"condition\"\n\n\nHowever, some errors might still occur:\n\nsquare_better('r')\n\n[1] NA\n\ntryCatch(\n  expr = square_better('r')\n  ,error = function(e) str(e)\n  ,warning = function(w) str(w)\n)\n\nList of 2\n $ message: chr \"attempting to convert input to numeric\"\n $ call   : language square_better(\"r\")\n - attr(*, \"class\")= chr [1:3] \"simpleWarning\" \"warning\" \"condition\"\n\n\nIn this case, the input can not be converted to a type compatible with the operation we want to perform, so there is not much further to do, therefore, the function needs to stop in a safe way and notify the user of the problem. Stopping in a safe way means halting the execution of the program without compromising the provided data or the following steps.\nLet’s see what would be an optimal way to write this function:\n\nsquare_best &lt;- function(x){\n  if(inherits(x,'character')) {\n    warning('attempting to convert input to numeric')\n    x &lt;- tryCatch(as.numeric(x)\n                  ,error = \\(e) cat('error of execution, evaluation returned ',e)\n                  ,finally = print('execution problem in square_best')\n                  )\n  }\n  return(x**2)\n}\n\n\nsquare_best('r')\n\n[1] \"execution problem in square_best\"\n\n\n[1] NA\n\n\nHere, we included a tryCatch call inside the function to respond to erroneous inputs. While if the input is good, no problems will be printed as the tryCatch call simply returns the desired value:\n\nsquare_best(3)\n\n[1] 9\n\n\nNo error !"
  },
  {
    "objectID": "posts/error_handling/error_handling_r.html#conclusion",
    "href": "posts/error_handling/error_handling_r.html#conclusion",
    "title": "Error handling in R",
    "section": "Conclusion",
    "text": "Conclusion\nThis post covers a basic example of a good approach to building fault proof functions on the example of the squaring operation."
  },
  {
    "objectID": "posts/spatial_index/spatial_index.html",
    "href": "posts/spatial_index/spatial_index.html",
    "title": "Index from scratch",
    "section": "",
    "text": "This post will go through the logic behind using indexation, give example algorithms and implementations for simple cases of spatial data. By the end of it, you will understand why is indexing such an important aspect of manipulating data, what are the most common indexing methods, and how to use them with spatial data.\n\n\n\n\n\nThere are many ways to index data in order to speed up the operations one might need to do with it. They are adapted for various tasks and types of data, this post will keep it simple and be an introductory walk into indexing for searching in spatial points. The most obvious operations one might need to do is search. In an unordered set with \\(N\\) elements, the complexity of finding a specific one scales with the size of the set. When studying the complexity of an algorithm, we are interested in knowing the worst case scenario performance under the implemented algorithm for a given parameter of the data, such as the size \\(N\\). We note the complexity of the algorithm as a function of the parameter with the big O notation \\(O(N)\\). The complexity of an algorithm can become a big expression when we start adding up all the individual contributions of operations as a function of \\(N\\). Therefore, only the term of dominant magnitude is kept. For example, if we have an algorithm that first visits each data point to check a condition and then performs an operation between all the pairs of data points, we could write the complexity as \\(O(N + N^2)\\), however the first element of this sum will be insignificant as \\(N\\) increases and therefore we only keep the second one, \\(O(N^2)\\). Example, we have 10 data points and we first validate the data by visiting each point individually (10 operations), then we compute the distance for each pair of points (10*10=100 multiplications). The total will be 110, but overall we are limited in our computing capabilities by the step that took us 100 operations. This becomes even more obvious as \\(N\\) increases, say for \\(N=100,1000…\\).\n\nsample_set &lt;- sample(1:100,10) |&gt; unique()\n\nsearch_value &lt;- sample(sample_set,1)\n\nsample_set\n\n [1] 71  9 16 31 95 63  5 69 11 18\n\n\nNow let’s consider the fact that the set we are looking at contains numbers, or what we really mean by that, is that it is ordered, therefore, we can arrange the elements in a specific way.\n\nsample_set &lt;- sample_set[order(sample_set)]\nsample_set\n\n [1]  5  9 11 16 18 31 63 69 71 95\n\n\nNow let’s try using this order in our search, say we look for a specific number like 9 and its location in the set, we might go on looking for other numbers that have a similar value, and we will expect the number we look for, if it is in the set, to be somewhere nearby. We can make a function to do this by breaking the set into 2 subsets, each contains respectively values that are greater or smaller than the median value of the set. We can easily get the median by looking at the middle of the set, since it is ordered.\n\nsubsets_list &lt;- list()\n\nset_median &lt;- median(sample_set)\n\nsubsets_list[[\"left\"]] &lt;- sample_set[sample_set&lt;set_median]\n\nsubsets_list[[\"right\"]] &lt;- sample_set[sample_set&gt;=set_median]\n\nsubsets_list\n\n$left\n[1]  5  9 11 16 18\n\n$right\n[1] 31 63 69 71 95\n\n\nwe call the left subset the values smaller than the median, and the right subset the values greater than the median. Depending on whether the number we look for is greater or smaller than the median, we will restrict our search to the left or right set.\n\nif(search_value&lt;set_median) {\n  updated_subset &lt;- list()\n  set_median &lt;- median(subsets_list[[\"left\"]])\n  \n  updated_subset[[\"left\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&lt;set_median]\n  updated_subset[[\"right\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&gt;=set_median]\n  \n  subsets_list &lt;- updated_subset\n  \n} else {\n  updated_subset &lt;- list()\n  set_median &lt;- median(subsets_list[[\"right\"]])\n  \n  updated_subset[[\"left\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&lt;set_median]\n  updated_subset[[\"right\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&gt;=set_median]\n  \n  subsets_list &lt;- updated_subset\n  \n}\n\nsubsets_list\n\n$left\n[1] 5 9\n\n$right\n[1] 11 16 18\n\n\nThe previous code cell shows how we perform a step in the search algorithm. At each step, we focus on a narrower interval with less data in it until we find our match. So this step can be repeated until we are left with only 1 single value, and if the value matches our search value, then we found it (!). If it does not match exactly our value, then we have found the approximate nearest neighbor of our value in the set. This is an important point especially in spatial data.\nTherefore, we can have a function that looks the following way to find a value in a set:\n\nin_ordered_set &lt;- function(set, x){\n  \n  # In case not ordered.\n  set &lt;- set[ordered(set)]\n  \n  # to tell the while loop to continue\n  found &lt;- FALSE\n  \n  set_median &lt;- median(set)\n  \n  subsets_list[[\"left\"]] &lt;- set[set &lt; set_median]\n  subsets_list[[\"right\"]] &lt;- set[set &gt;= set_median]\n  \n  while(!found) {\n    if(x&lt;set_median) {\n      \n      if(length(subsets_list[[\"left\"]])==1 && subsets_list[[\"left\"]][1]==x) return(TRUE)\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"left\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n      \n    } else {\n      \n      if(length(subsets_list[[\"right\"]])==1 && subsets_list[[\"right\"]][1]==x) return(TRUE)\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"right\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n    }\n  }\n  return(found)\n}\n\nAnd applied to this example data:\n\nin_ordered_set(set = sample_set,x=search_value)\n\n[1] TRUE\n\n\nAnd for the nearest neighbor value, we simply return the final value we found at the end of the iteration, so with minor changes, the function looks like that:\n\nnearest_neighbour &lt;- function(set, x){\n  # In case not ordered.\n  set &lt;- set[ordered(set)]\n  \n  found &lt;- FALSE\n  \n  subsets_list &lt;- list()\n  \n  set_median &lt;- median(set)\n  subsets_list[[\"left\"]] &lt;- set[set&lt;set_median]\n  subsets_list[[\"right\"]] &lt;- set[set&gt;=set_median]\n  \n  while(!found) {\n    if(x&lt;set_median) {\n      \n      # this will break the while loop\n      if(length(subsets_list[[\"left\"]])==1) return(subsets_list[[\"left\"]][1])\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"left\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n      \n    } else {\n      \n      # this will break the while loop\n      if(length(subsets_list[[\"right\"]])==1) return(subsets_list[[\"right\"]][1])\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"right\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n    }\n  }\n  # this is not actually required, as unless there is an error in input, the function will return from inside the while loop \n  return(NULL)\n}\n\nAnd the nearest neighbor of our search value, a random number between \\(0\\) and \\(100\\) for example could be:\n\nsearch_nn_value &lt;- sample(1:100,1)\n\nnearest_neighbour(set=sample_set, x=search_nn_value)\n\n[1] 31\n\n\nThis algorithm is actually called the approximate nearest neighbor search, because is some cases when the search value is very close to the median of one of the intervals, it can give the second nearest neighbor instead.\n\n\nNow going back to the discussion on algorithm complexity, how can we measure the complexity of finding an element in an ordered set by this method ? At each step, we divide the original set by 2, until we are left with a single value. In other words, how many time do we need to divide the length of the set by 2 to get one. This value is the logarithm of base 2 of the size of the set, and it’s the maximum number of step this algorithm can take to find something, so \\(O(N)=log_2(N)\\).\nSo why would we do this ? Because the logarithm grows much slower than the linear function, and therefore, we will find elements much faster in an ordered set using this method instead. Here is both curves visualised:\n\nx &lt;- seq(from = 1, to = 1e6, by = 1000)\nx_2 &lt;- seq(from = 1, to = 20)\n\nplot(x_2,x_2,cex=.3,xlab = \"N\", ylab = \"Time\")\nlines(x_2,log2(1+x_2),col=\"darkred\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinear\nlog2\n\n\n\n\n996\n995001\n19.92434\n\n\n997\n996001\n19.92579\n\n\n998\n997001\n19.92724\n\n\n999\n998001\n19.92868\n\n\n1000\n999001\n19.93013\n\n\n\n\n\nIn the worst case scenario for the linear algorithm, it would require almost a million steps to find our value, while with the ordered set and adapted algorithm, it would be 20 steps !!! The sizes of data that we manipulate regularly are in the thousands and even sometimes millions of rows, so imagine the performance boost we get from using adapted algorithms. The method that was just described is one of the most famous search algorithms called the Binary Search Tree, or B-tree.\n\nB-Tree on wikipedia"
  },
  {
    "objectID": "posts/spatial_index/spatial_index.html#algorithm-complexity-on",
    "href": "posts/spatial_index/spatial_index.html#algorithm-complexity-on",
    "title": "Index from scratch",
    "section": "",
    "text": "There are many ways to index data in order to speed up the operations one might need to do with it. They are adapted for various tasks and types of data, this post will keep it simple and be an introductory walk into indexing for searching in spatial points. The most obvious operations one might need to do is search. In an unordered set with \\(N\\) elements, the complexity of finding a specific one scales with the size of the set. When studying the complexity of an algorithm, we are interested in knowing the worst case scenario performance under the implemented algorithm for a given parameter of the data, such as the size \\(N\\). We note the complexity of the algorithm as a function of the parameter with the big O notation \\(O(N)\\). The complexity of an algorithm can become a big expression when we start adding up all the individual contributions of operations as a function of \\(N\\). Therefore, only the term of dominant magnitude is kept. For example, if we have an algorithm that first visits each data point to check a condition and then performs an operation between all the pairs of data points, we could write the complexity as \\(O(N + N^2)\\), however the first element of this sum will be insignificant as \\(N\\) increases and therefore we only keep the second one, \\(O(N^2)\\). Example, we have 10 data points and we first validate the data by visiting each point individually (10 operations), then we compute the distance for each pair of points (10*10=100 multiplications). The total will be 110, but overall we are limited in our computing capabilities by the step that took us 100 operations. This becomes even more obvious as \\(N\\) increases, say for \\(N=100,1000…\\).\n\nsample_set &lt;- sample(1:100,10) |&gt; unique()\n\nsearch_value &lt;- sample(sample_set,1)\n\nsample_set\n\n [1] 71  9 16 31 95 63  5 69 11 18\n\n\nNow let’s consider the fact that the set we are looking at contains numbers, or what we really mean by that, is that it is ordered, therefore, we can arrange the elements in a specific way.\n\nsample_set &lt;- sample_set[order(sample_set)]\nsample_set\n\n [1]  5  9 11 16 18 31 63 69 71 95\n\n\nNow let’s try using this order in our search, say we look for a specific number like 9 and its location in the set, we might go on looking for other numbers that have a similar value, and we will expect the number we look for, if it is in the set, to be somewhere nearby. We can make a function to do this by breaking the set into 2 subsets, each contains respectively values that are greater or smaller than the median value of the set. We can easily get the median by looking at the middle of the set, since it is ordered.\n\nsubsets_list &lt;- list()\n\nset_median &lt;- median(sample_set)\n\nsubsets_list[[\"left\"]] &lt;- sample_set[sample_set&lt;set_median]\n\nsubsets_list[[\"right\"]] &lt;- sample_set[sample_set&gt;=set_median]\n\nsubsets_list\n\n$left\n[1]  5  9 11 16 18\n\n$right\n[1] 31 63 69 71 95\n\n\nwe call the left subset the values smaller than the median, and the right subset the values greater than the median. Depending on whether the number we look for is greater or smaller than the median, we will restrict our search to the left or right set.\n\nif(search_value&lt;set_median) {\n  updated_subset &lt;- list()\n  set_median &lt;- median(subsets_list[[\"left\"]])\n  \n  updated_subset[[\"left\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&lt;set_median]\n  updated_subset[[\"right\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&gt;=set_median]\n  \n  subsets_list &lt;- updated_subset\n  \n} else {\n  updated_subset &lt;- list()\n  set_median &lt;- median(subsets_list[[\"right\"]])\n  \n  updated_subset[[\"left\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&lt;set_median]\n  updated_subset[[\"right\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&gt;=set_median]\n  \n  subsets_list &lt;- updated_subset\n  \n}\n\nsubsets_list\n\n$left\n[1] 5 9\n\n$right\n[1] 11 16 18\n\n\nThe previous code cell shows how we perform a step in the search algorithm. At each step, we focus on a narrower interval with less data in it until we find our match. So this step can be repeated until we are left with only 1 single value, and if the value matches our search value, then we found it (!). If it does not match exactly our value, then we have found the approximate nearest neighbor of our value in the set. This is an important point especially in spatial data.\nTherefore, we can have a function that looks the following way to find a value in a set:\n\nin_ordered_set &lt;- function(set, x){\n  \n  # In case not ordered.\n  set &lt;- set[ordered(set)]\n  \n  # to tell the while loop to continue\n  found &lt;- FALSE\n  \n  set_median &lt;- median(set)\n  \n  subsets_list[[\"left\"]] &lt;- set[set &lt; set_median]\n  subsets_list[[\"right\"]] &lt;- set[set &gt;= set_median]\n  \n  while(!found) {\n    if(x&lt;set_median) {\n      \n      if(length(subsets_list[[\"left\"]])==1 && subsets_list[[\"left\"]][1]==x) return(TRUE)\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"left\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n      \n    } else {\n      \n      if(length(subsets_list[[\"right\"]])==1 && subsets_list[[\"right\"]][1]==x) return(TRUE)\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"right\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n    }\n  }\n  return(found)\n}\n\nAnd applied to this example data:\n\nin_ordered_set(set = sample_set,x=search_value)\n\n[1] TRUE\n\n\nAnd for the nearest neighbor value, we simply return the final value we found at the end of the iteration, so with minor changes, the function looks like that:\n\nnearest_neighbour &lt;- function(set, x){\n  # In case not ordered.\n  set &lt;- set[ordered(set)]\n  \n  found &lt;- FALSE\n  \n  subsets_list &lt;- list()\n  \n  set_median &lt;- median(set)\n  subsets_list[[\"left\"]] &lt;- set[set&lt;set_median]\n  subsets_list[[\"right\"]] &lt;- set[set&gt;=set_median]\n  \n  while(!found) {\n    if(x&lt;set_median) {\n      \n      # this will break the while loop\n      if(length(subsets_list[[\"left\"]])==1) return(subsets_list[[\"left\"]][1])\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"left\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"left\"]][subsets_list[[\"left\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n      \n    } else {\n      \n      # this will break the while loop\n      if(length(subsets_list[[\"right\"]])==1) return(subsets_list[[\"right\"]][1])\n      \n      updated_subset &lt;- list()\n      set_median &lt;- median(subsets_list[[\"right\"]])\n      updated_subset[[\"left\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&lt;set_median]\n      updated_subset[[\"right\"]] &lt;- subsets_list[[\"right\"]][subsets_list[[\"right\"]]&gt;=set_median]\n      \n      subsets_list &lt;- updated_subset\n    }\n  }\n  # this is not actually required, as unless there is an error in input, the function will return from inside the while loop \n  return(NULL)\n}\n\nAnd the nearest neighbor of our search value, a random number between \\(0\\) and \\(100\\) for example could be:\n\nsearch_nn_value &lt;- sample(1:100,1)\n\nnearest_neighbour(set=sample_set, x=search_nn_value)\n\n[1] 31\n\n\nThis algorithm is actually called the approximate nearest neighbor search, because is some cases when the search value is very close to the median of one of the intervals, it can give the second nearest neighbor instead.\n\n\nNow going back to the discussion on algorithm complexity, how can we measure the complexity of finding an element in an ordered set by this method ? At each step, we divide the original set by 2, until we are left with a single value. In other words, how many time do we need to divide the length of the set by 2 to get one. This value is the logarithm of base 2 of the size of the set, and it’s the maximum number of step this algorithm can take to find something, so \\(O(N)=log_2(N)\\).\nSo why would we do this ? Because the logarithm grows much slower than the linear function, and therefore, we will find elements much faster in an ordered set using this method instead. Here is both curves visualised:\n\nx &lt;- seq(from = 1, to = 1e6, by = 1000)\nx_2 &lt;- seq(from = 1, to = 20)\n\nplot(x_2,x_2,cex=.3,xlab = \"N\", ylab = \"Time\")\nlines(x_2,log2(1+x_2),col=\"darkred\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlinear\nlog2\n\n\n\n\n996\n995001\n19.92434\n\n\n997\n996001\n19.92579\n\n\n998\n997001\n19.92724\n\n\n999\n998001\n19.92868\n\n\n1000\n999001\n19.93013\n\n\n\n\n\nIn the worst case scenario for the linear algorithm, it would require almost a million steps to find our value, while with the ordered set and adapted algorithm, it would be 20 steps !!! The sizes of data that we manipulate regularly are in the thousands and even sometimes millions of rows, so imagine the performance boost we get from using adapted algorithms. The method that was just described is one of the most famous search algorithms called the Binary Search Tree, or B-tree.\n\nB-Tree on wikipedia"
  },
  {
    "objectID": "posts/tennis_map/tennis_map.html",
    "href": "posts/tennis_map/tennis_map.html",
    "title": "Using open source data for web maps",
    "section": "",
    "text": "This post will cover the steps required to get data from an open source, analyze it, and then publish it as a interactive web document. The problem we will be trying to solve is to find all the tennis courts in London that are mapped in OpenStreetMap and show them online so that we could refer to it when we want to play. As there are multiple services that allow to book courts, and some courts are in free access, it is sometimes difficult to find the best spot to play at."
  },
  {
    "objectID": "posts/tennis_map/tennis_map.html#nodes-ways-relations",
    "href": "posts/tennis_map/tennis_map.html#nodes-ways-relations",
    "title": "Using open source data for web maps",
    "section": "Nodes, ways, relations",
    "text": "Nodes, ways, relations\nThis is specific to how OSM stores any geographic feature. More on this can be found here. Basically, any feature of the map is constructed out of nodes, which can also be grouped into ways (the name is slightly confusing as it refers something more general, than a road, but rather to a set on nodes). A road segment for example can be identified as a way, which will have a set of nodes that locate the start and end of each individual straight line segment that constitutes a road. Each node can be independent as well. In the case of a road sign, it will simply be identified as a node. Both nodes and ways will have their set of tags. This includes the key and value that we looked at earlier, but also any other information that is known about the feature, such as its coordinates etc…"
  },
  {
    "objectID": "posts/tennis_map/tennis_map.html#processing",
    "href": "posts/tennis_map/tennis_map.html#processing",
    "title": "Using open source data for web maps",
    "section": "Processing",
    "text": "Processing\nNow that the data is downloaded, we will need to process it first in order to extract the relevant information, in our case the location of tennis courts in London. We will use a very powerful command line tool called osmosis. It allows to work on raw OSM data sets of pretty much any size and efficiently extract what we need from it. Follow the instruction on its wiki on how to download it.\nNow open a terminal and type osmosis into it to check that your machine has set it up successfully. You should see a brief tutorial show up, which might be helpful to look at as a first example.\nWe will now create the command that will go through the raw data and take out what we ask it. After calling the osmosis library, we need to specify the data we are working on with the --read-pbf command, and then we add the parameters explaining what to we want to extract. We will start by specifying a bounding box with the --bounding-box parameter to make sure we are extracting in the right area. Then, we add the keys and values we are interested in. This is done with the command --tag-filter accept-ways sport=tennis in which we add the ways we want accept, in other words want to extract. We will omit the relations by specifying --tag-filter reject-relations. Then we specify that we want to extract the nodes that are used in the ways as well with --used-nodes. The final information we need to provide is what to do with the output, in our case, we will write it locally into a XML file with the --write-xml command.\nThe final command looks like that:\n\n\nosmosis \\\n--read-pbf data/london.osm.pbf \\\n--bounding-box left=-0.5507 right=0.2994 top=51.7168 bottom=51.2499 \\\n--tag-filter accept-ways sport=tennis \\\n--tag-filter reject-relations \\\n--used-nodes \\\n--write-xml data/london_tennis.osm\n\nOsmosis also allows to write short versions of commands to simplify the process, in our case, we can rewrite to have:\n\n\nosmosis \\\n--rb data/london.osm.pbf \\\n--bb left=-0.5507 right=0.2994 top=51.7168 bottom=51.2499 \\\n--tf accept-ways sport=tennis \\\n--tf reject-relations \\\n--un \\\n--wx data/london_tennis.osm\n\nNotice the escape character \\ that allows to write other multiple lines, making it easy to read. When the command is done executing you should have a file london_tennis.osm in the data folder, it should weight around 5.5 MB.\nWe can now open this fie to have a preview and better understanding of the the structure of the file. We can see the node elements, and within, the tags lat, lon corresponding to their coordinates as well as an id. If we scroll further down, we see appearing ways that contain node ids and other tags, such as sport, leisure, tennis, name."
  },
  {
    "objectID": "posts/tennis_map/tennis_map.html#engineering",
    "href": "posts/tennis_map/tennis_map.html#engineering",
    "title": "Using open source data for web maps",
    "section": "Engineering",
    "text": "Engineering\nWe have now narrowed down our data, but there is still some more to do to be able to use it for a web map. FOr this section, we will R to read in the XML files and further process the data.\n\nReading XML data in R\nThe XML package in R allows us to manipulate such data files with the user friendliness of R. We will read in the file, and define a few functions to assist us in extracting the attributes that we will use:\n\nlibrary(sf)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(XML)\nlibrary(foreach)\nlibrary(Btoolkit)\nlibrary(xml2)\nlibrary(leaflet)\nlibrary(leafgl)\n\n####\n\nget_nodes &lt;- function(el, name = \"nd\", recursive = FALSE) {\n  kids = xmlChildren(el)\n  idx = (names(kids) == name)\n  els = kids[idx]\n  # if (!recursive || xmlSize(el) == 0) \n  #   return(els)\n  # subs = xmlApply(el, xmlElementsByTagName, name, TRUE)\n  # subs = unlist(subs, recursive = FALSE)\n  # # append.xmlNode(els, subs[!sapply(subs, is.null)])\n  sapply(els, XML::xmlAttrs, USE.NAMES = FALSE) |&gt; unname()\n}\n\nget_tags &lt;- function(el, name = \"tag\", recursive = FALSE) {\n  kids = xmlChildren(el)\n  idx = (names(kids) == name)\n  els = kids[idx]\n  # if (!recursive || xmlSize(el) == 0) \n  #   return(els)\n  # subs = xmlApply(el, xmlElementsByTagName, name, TRUE)\n  # subs = unlist(subs, recursive = FALSE)\n  # # append.xmlNode(els, subs[!sapply(subs, is.null)])\n  sapply(els,XML::xmlAttrs) |&gt; \n    unname() |&gt; \n    t() |&gt; \n    as.data.table() |&gt; \n    `names&lt;-`(c(\"key\",\"value\"))\n  \n}\n\n#####\n\nlondon_tennis_xml &lt;- XML::xmlParse(\"tennis_map/data/london_tennis.osm.xml\")\n\n\n\nNodes\nNow we can start extracting the nodes:\n\nnodes &lt;- getNodeSet(london_tennis_xml,\"//node\") # //node means we want only node tags from the XML\n\n# this function is applied to each separate //node element and gets its attributes. \nnodes &lt;- xmlApply(nodes, xmlAttrs) \n\n# reformat the data into a data table with id, lon, lat variables. \nnodes_dt &lt;- sapply(nodes, FUN = function(nd) c(id = nd[[\"id\"]],lon = nd[[\"lon\"]],lat = nd[[\"lat\"]])) |&gt; \n  t() |&gt; \n  as.data.table() \n\n# nodes_dt\nnodes_dt[,id:=as.character(id)] # make sure ids are stored as character\n\n\n\nWays\nNow the ways, it is a bit more complicated here, because each way contains a potentially different number of nodes and key-value pairs. To help us extract the data properly, we defined the functions earlier.\n\nways &lt;-\n  XML::xpathApply(london_tennis_xml, \"//way\", fun = function(w) {w})\n\n# get_tags(ways[[1]])\n\nway_nodes &lt;- lapply(ways, FUN = function(w) {list(id = XML::xmlGetAttr(w,\"id\")\n                                                      ,nodes = get_nodes(w)\n                                                      ,tags = get_tags(w)\n                                                  )\n})\n\nLet’s have a look at all the unique keys that we extracted:\n\nsapply(way_nodes, FUN = function(way) way$tags$key) |&gt; unlist() |&gt; unique()\n\nNow we will add the coordinates of the nodes to each way.\n\nfor (j in 1:length(way_nodes)) {\n  way_nodes[[j]]$nodes &lt;- cbind(way_nodes[[j]]$nodes,nodes_dt[match(way_nodes[[j]]$nodes,id),.(lon,lat)])\n}\n\nFinally, let’s create polygons for each way corresponding to a single court, or a sports facility containing tennis courts.\n\ntennis_courts &lt;- foreach::foreach(i = 1:length(way_nodes)) %do% {\n  nds &lt;- way_nodes[[i]]$nodes[,.(as.double(lon),as.double(lat))] |&gt; \n    as.matrix()\n  if(any(is.na(nds))) sf::st_polygon(x = list(),dim = \"XY\") else sf::st_polygon(x = list(nds),dim = \"XY\")\n}\n\ntennis_courts &lt;- tennis_courts |&gt; st_sfc(crs = 4326) |&gt; st_as_sf()\n\n\n\nPopup\nNow with the key-value pairs containing information about the courts we can create character containint HTML that will be shown as a popup on our map.\n\npopups &lt;- sapply(way_nodes, function(pitch) paste(\"&lt;h4&gt;\"\n                                                  ,pitch$tags$key\n                                                  ,\" : \"\n                                                  ,pitch$tags$value\n                                                  ,\"&lt;/h4&gt;\"\n                                                  ,sep = \"\"\n                                                  ,collapse = \" \"))\n\n\n####\n\ntennis_courts$popups &lt;- popups\n\nFinally, we save the data as a geojson file in a data folder that we will use for our webpage."
  },
  {
    "objectID": "posts/regex/reg_expressions.html",
    "href": "posts/regex/reg_expressions.html",
    "title": "Regular expressions",
    "section": "",
    "text": "This tutorial will introduce the basics of regular expressions, through general considerations and then concrete examples of varying difficulty."
  },
  {
    "objectID": "posts/regex/reg_expressions.html#categories-of-characters",
    "href": "posts/regex/reg_expressions.html#categories-of-characters",
    "title": "Regular expressions",
    "section": "Categories of characters",
    "text": "Categories of characters\nThe following table presents a non-exhaustive set of character categories:\n\n\n\n\n\n\n\n\n\n\n\nCategory\nSymbol\nExample\nOpposite\nSymbol\nExample\n\n\n\n\nnumbers\n\\d\n12387\nnon-numbers\n\\D\n:-dfv*\n\n\nletters (lower case)\n[a-z]\njsknvs\nletters (upper case)\n[A-Z]\nSDFVM\n\n\nletters (lower & upper)\n[a-zA-Z]\nfvFVDf\n-\n-\n-\n\n\nnumbers & letters\n[:alphanum:]\n3k4rF4\n-\n-\n-\n\n\nwords\n\\w\nHello\nnon-words\n\\W\n!\n\n\nwhitespace\n\\s\n” ”\nnon whitespace\n\\S\n“kj4&n”\n\n\nboundaries of words\n\\b\n_hello_\ninterior of words\n\\B\nh_e_l_l_o\n\n\n\nAs you can see, some categories have a designated letter, but in other cases they can be designated by placing the characters of interest between square braquets. This allows to customize and create your own categories of regexes suitable for your specific needs. For example, let’s say you want to find numbers or the letter h, this would look like [\\dh]. Inversely, if you want to match for everything except those characters, place the ^ in front of the others: [^\\dh].\nOther symbols help us with specifying repetitions, or ordering:\n\nPositioning\n\n^ : beginning of sequence, ^\\d\n\\$ : end of sequence, \\dh\\$\n\nRepetition\n\n* : zero or more times\n+ : one or more time\n{min, max} : minimal and maximal number of repetitions\n{n} : match exactly n times\n\nAny symbol\n\n. : match any symbol\n? : match potentially a symbol\n\nGroups\n\n() : group elements\n\nLogical\n\n| : logical or\n\n\nIt is possible to group several characters by enclosing them into round brackets(). This will treat whatever is inside them as one single block of characters. We can include any of the mentioned regex symbols within them, particularly the logical or that can help with differentiating cases."
  },
  {
    "objectID": "posts/michelin_web_scrapping/michelin_data_engineering.html",
    "href": "posts/michelin_web_scrapping/michelin_data_engineering.html",
    "title": "Web scrapping in R",
    "section": "",
    "text": "Sometime, you can find interesting data for a project on the web, and quite often it will be possible to download the raw data set,like a spreadsheet, csv or json, for your own use. However, this is not always the case. And in these kind of cases, there is always the possibility to try web-scrapping. While it can seem like a lengthy extra step towards the realization of your intended idea, it can also give you the opportunity to develop a new skill, and understand better how websites work in general.\nWeb scrapping is generally legal, so you shouldn’t feel like you are doing something wrong when scrapping data. Occasionally, some web sites will have protection against it, but you still can manage to get what you want. This post will show an example of using the rvest package ( another great package from the tidyverse team) to scrap data from the website of michelin restaurants. Their data is not accessible online in raw format, so we have a true real-world case in hands."
  },
  {
    "objectID": "posts/michelin_web_scrapping/michelin_data_engineering.html#steps",
    "href": "posts/michelin_web_scrapping/michelin_data_engineering.html#steps",
    "title": "Web scrapping in R",
    "section": "Steps",
    "text": "Steps\nFirst, we read the whole html code of the page via the read_html function, then we specify the class of the element we want to read from. Second, we specify whether we want the whole content of the element or just the text contained inside of it. This is done with the functions html_text2 or html_text respectively. We will be using the first one. Additionally, we can get attributes of the elements, this is done through the html_attr function. For each of these function, we specify as argument the attribute of interest found earlier. Finally, we need to combine these function in order to get the data, this is done by chaining the operations with the pipe operator,|&gt;, from tidyverse.\n\nReading the website\n\nmichelin_url &lt;- \"https://guide.michelin.com/gb/en/restaurants\"\n\nhtml &lt;- read_html(michelin_url)\n\n\n\nExtracting the relevant elements\n\nname &lt;- html |&gt;\n  html_elements(\".card__menu-content--title\") |&gt; \n  html_text2()\n\nThis has created a vector containing the text of all the elements that were found under the class specified, .card__menu-content--title.\nWe can now perform the same steps for each value of interest for a restaurant and group it into a tibble.\n\nname &lt;- html |&gt;\n  html_elements(\".card__menu-content--title\") |&gt;\n  html_text2()\n\nstyle &lt;- html |&gt;\n  html_elements(\".card__menu-footer--price\") |&gt;\n  html_text2() \n\nlat &lt;- html |&gt;\n  html_elements(\".card__menu\") |&gt;\n  html_attr(\"data-lat\")\n\nlng &lt;- html |&gt;\n  html_elements(\".card__menu\") |&gt;\n  html_attr(\"data-lng\")\n\npre_link &lt;- html |&gt;\n  html_elements(\".link\") |&gt;\n  html_attr(\"href\")\n\npre_link &lt;- paste(\"https://guide.michelin.com\",pre_link[-c(1:18)],sep = \"\")"
  },
  {
    "objectID": "posts/michelin_web_scrapping/michelin_data_engineering.html#putting-it-together",
    "href": "posts/michelin_web_scrapping/michelin_data_engineering.html#putting-it-together",
    "title": "Web scrapping in R",
    "section": "Putting it together",
    "text": "Putting it together\nNext, the individual variables are combined in a single data set.\n\nmichelin_guide_scrap &lt;- data.frame(name\n                                   ,\"lat\"=as.numeric(lat)\n                                   ,\"lng\"=as.numeric(lng)\n                                   ,style\n                                   ,pre_link[-1])\n\nNote that the link that we extract is the relative path of the page, so we paste the missing bit of the absolute path before it.\nNow, let’s visualize the result:\n\n\n\nScrapped data\n\n\n\n\n\n\n\n\n\nname\nlat\nlng\nstyle\npre_link..1.\n\n\n\n\nŠug\n43.50949\n16.44329\n€€€ · Regional Cuisine\nhttps://guide.michelin.com/gb/en/split-dalmatia/split/restaurant/sug\n\n\nKonoba Tri Piruna\n43.75935\n15.77718\n€€ · Modern Cuisine\nhttps://guide.michelin.com/gb/en/sibenik-knin/vodice/restaurant/konoba-tri-piruna\n\n\nTaj Mahal\n42.64050\n18.10876\n€€ · Balkan\nhttps://guide.michelin.com/gb/en/dubrovnik-neretva/dubrovnik/restaurant/taj-mahal\n\n\nBeštija\n45.81079\n15.97294\n€€ · Market Cuisine\nhttps://guide.michelin.com/gb/en/zagreb-region/zagreb/restaurant/bestija\n\n\nIl Ponte\n43.51908\n16.25026\n€€€ · Contemporary\nhttps://guide.michelin.com/gb/en/split-dalmatia/trogir/restaurant/il-ponte\n\n\nKonoba Škoj\n43.37098\n16.35171\n€€ · Mediterranean Cuisine\nhttps://guide.michelin.com/gb/en/split-dalmatia/isola-di-solta/restaurant/konoba-skoj\n\n\n\n\n\nEt voilà !\nThe next steps now could be to look at each restaurants dedicated page and see if there is some more info that can be scrapped from there ! Additionally, there are almost 800 pages on the website that list all the restaurants the michelin guide has reviewed, so there is some need to build extra functions to automate the scrapping of all the data."
  }
]